# MCP Registry Search

Semantic search API for MCP (Model Context Protocol) servers using hybrid search (BM25 + embeddings).

## Features

- ðŸ” **Hybrid search** combining lexical (PostgreSQL full-text) and semantic (pgvector) search
- ðŸš€ **Fast vector similarity** using OpenAI embeddings + Supabase pgvector
- ðŸ“Š **Ranked results** using weighted scoring
- ðŸ”„ **Automatic ETL pipeline** to fetch and index MCP servers
- ðŸŒ **FastAPI REST API** for web access
- ðŸ”Œ **FastMCP server** for MCP client integration
- â˜ï¸ **Deployable to Vercel** (FastAPI) and any MCP-compatible host (FastMCP)

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MCP Registry   â”‚
â”‚   (Source API)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ ETL Pipeline
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Supabase     â”‚
â”‚  (PostgreSQL +  â”‚
â”‚    pgvector)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Search Query
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI REST  â”‚  FastMCP Server â”‚
â”‚      (Web)      â”‚   (MCP Clients) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Setup

### 1. Install dependencies

```bash
cd registry
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install -e .
```

### 2. Set up Supabase

1. Create a [Supabase account](https://supabase.com)
2. Create a new project
3. Run the SQL in `schema.sql` in the Supabase SQL editor
4. Get your project URL and anon key from Settings > API

### 3. Configure environment variables

```bash
cp .env.example .env
```

Edit `.env` and add:
- `OPENAI_API_KEY`: Your OpenAI API key
- `SUPABASE_URL`: Your Supabase project URL
- `SUPABASE_KEY`: Your Supabase anon key

### 4. Run ETL to fetch and index servers

```bash
python etl.py
```

This will:
1. Fetch all servers from the MCP registry
2. Filter to active servers (latest versions only)
3. Generate embeddings using OpenAI
4. Store in Supabase with vector indices

### 5. Start the servers

**FastAPI (REST API):**
```bash
uvicorn api:app --reload
```

API available at `http://localhost:8000`
- Docs: `http://localhost:8000/docs`

**FastMCP (MCP Server):**
```bash
python mcp_server.py
```

## API Usage

### REST API

**Search servers:**
```bash
curl "http://localhost:8000/search?q=kubernetes&limit=5"
```

**List all servers:**
```bash
curl "http://localhost:8000/servers?limit=100&offset=0"
```

**Health check:**
```bash
curl "http://localhost:8000/health"
```

### MCP Server

The FastMCP server provides:

**Tools:**
- `search_mcp_servers(query, limit, full_text_weight, semantic_weight)` - Search servers
- `list_mcp_servers(limit, offset)` - List all servers

**Resources:**
- `mcp-registry://search/{query}` - Search results as formatted text

**Prompts:**
- `find_mcp_server(task)` - Prompt template to find servers for a task

**Add to your MCP client config:**
```json
{
  "mcpServers": {
    "registry-search": {
      "command": "python",
      "args": ["/path/to/registry/mcp_server.py"],
      "env": {
        "OPENAI_API_KEY": "your-key",
        "SUPABASE_URL": "your-url",
        "SUPABASE_KEY": "your-key"
      }
    }
  }
}
```

## Deployment

### Vercel (FastAPI)

1. Install Vercel CLI:
```bash
npm i -g vercel
```

2. Add environment variables to Vercel:
```bash
vercel env add OPENAI_API_KEY
vercel env add SUPABASE_URL
vercel env add SUPABASE_KEY
```

3. Deploy:
```bash
vercel
```

### FastMCP Server

The FastMCP server can be:
- Run locally and connected via stdio
- Hosted on any server with Python support
- Deployed as an SSE or HTTP endpoint (change `transport` in `mcp_server.py`)

## ETL Updates

Run the ETL periodically to keep the index fresh:

```bash
# Set up a cron job
0 0 * * * cd /path/to/registry && python etl.py
```

Or use Vercel Cron Jobs, GitHub Actions, or any scheduler.

## Cost Estimates

**OpenAI Embeddings:**
- ~500 servers Ã— 1536 dimensions = ~$0.01 per full ETL run
- Query embeddings: ~$0.00001 per search

**Supabase:**
- Free tier: 500MB database + 2GB bandwidth
- Should be sufficient for MCP registry (~500 servers)

**Vercel:**
- Free tier: 100GB bandwidth
- Serverless function invocations included

## Development

**Project structure:**
```
registry/
â”œâ”€â”€ api.py              # FastAPI REST API
â”œâ”€â”€ mcp_server.py       # FastMCP server
â”œâ”€â”€ search.py           # Search engine
â”œâ”€â”€ etl.py              # ETL pipeline
â”œâ”€â”€ schema.sql          # Supabase schema
â”œâ”€â”€ pyproject.toml      # Dependencies
â”œâ”€â”€ vercel.json         # Vercel config
â””â”€â”€ README.md           # This file
```

## License

Same as parent project (Apache 2.0)